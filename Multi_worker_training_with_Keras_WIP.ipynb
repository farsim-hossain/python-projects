{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multi worker training with Keras .ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM/ltPzREgNg44RXwgJSQMN"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAQPBhBijNkb"
      },
      "source": [
        "# Multi worker training with Keras API \n",
        "tf.distribute.MultiWorkerMirroredStrategy. With the help of this strategy, a Keras model that was designed to run on single-worker can seamlessly work on multiple workers with minimal code change."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npj1GSBji4P1"
      },
      "source": [
        "import json \n",
        "import os \n",
        "import sys "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NSb0-oFjeXf"
      },
      "source": [
        "Before importing TensorFlow, make a few changes to the environment.\n",
        "\n",
        "Disable all GPUs. This prevents errors caused by the workers all trying to use the same GPU. For a real application each worker would be on a different machine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TO5GWjBsjYRC"
      },
      "source": [
        "# disable all GPUs\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSshO9aAjvGq"
      },
      "source": [
        "Reset the TF_CONFIG environment variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfTEuw77jva5"
      },
      "source": [
        "os.environ.pop('TF_CONFIG', None)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OLR3EKhkA8f"
      },
      "source": [
        "Be sure that the current directory is on python's path. This allows the notebook to import the files written by %%writefile later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiNkGuN-j1yC"
      },
      "source": [
        "if '.' not in sys.path:\n",
        "  sys.path.insert(0, '.')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E71wL6r5kJTt"
      },
      "source": [
        "# now import tf \n",
        "import tensorflow as tf"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeIjgvd8kPbD"
      },
      "source": [
        "## Dataset and model definition\n",
        "\n",
        "Next create an mnist.py file with a simple model and dataset setup. This python file will be used by the worker-processes in this tutorial:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0PwgbhAkNoS",
        "outputId": "431975f5-2dc1-4d52-d1b3-c83f0021491b"
      },
      "source": [
        "%%writefile mnist.py\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def mnist_dataset(batch_size):\n",
        "  (x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
        "  # The `x` arrays are in uint8 and have values in the range [0, 255].\n",
        "  # You need to convert them to float32 with values in the range [0, 1]\n",
        "  x_train = x_train / np.float32(255)\n",
        "  y_train = y_train.astype(np.int64)\n",
        "  train_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "      (x_train, y_train)).shuffle(60000).repeat().batch(batch_size)\n",
        "  return train_dataset\n",
        "\n",
        "def build_and_compile_cnn_model():\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.Input(shape=(28, 28)),\n",
        "      tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
        "      tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
        "      \n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dense(128, activation='relu'),\n",
        "      tf.keras.layers.Dense(10)\n",
        "  ])\n",
        "  model.compile(\n",
        "      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "      optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n",
        "      metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting mnist.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xIrFXdXnNGz"
      },
      "source": [
        "Try training the model for a small number of epochs and observe the results of a single worker to make sure everything works correctly. As training progresses, the loss should drop and the accuracy should increase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFjUiS43nGJE",
        "outputId": "a2aaa624-5407-4ff2-b99f-c1cdfc8e08f7"
      },
      "source": [
        "import mnist\n",
        "\n",
        "batch_size = 64\n",
        "single_worker_dataset = mnist.mnist_dataset(batch_size)\n",
        "single_worker_model = mnist.build_and_compile_cnn_model()\n",
        "single_worker_model.fit(single_worker_dataset, epochs=3, steps_per_epoch=70)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
            "Epoch 1/3\n",
            "70/70 [==============================] - 4s 45ms/step - loss: 2.2826 - accuracy: 0.1529\n",
            "Epoch 2/3\n",
            "70/70 [==============================] - 3s 43ms/step - loss: 2.2441 - accuracy: 0.3431\n",
            "Epoch 3/3\n",
            "70/70 [==============================] - 3s 43ms/step - loss: 2.2044 - accuracy: 0.4935\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f881ddbc650>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIL0k_YerkmL"
      },
      "source": [
        "## Multi workder configuration \n",
        "\n",
        "We need TF.CONFIG env variable for training on multiple machines "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCYzKrHTnqe-"
      },
      "source": [
        "tf_config = {\n",
        "    'cluster': {\n",
        "        'worker' : ['localhost:12345', 'localhost: 23456']\n",
        "    },\n",
        "    'task': {'type': 'worker', 'index': 0}\n",
        "}"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "DiMDrQ_2sVOr",
        "outputId": "9a6e06e4-065c-4343-f55e-2952ba6f8c2b"
      },
      "source": [
        "# tf_config as a json string \n",
        "json.dumps(tf_config)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'{\"cluster\": {\"worker\": [\"localhost:12345\", \"localhost: 23456\"]}, \"task\": {\"type\": \"worker\", \"index\": 0}}'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuPyp2fSsp43"
      },
      "source": [
        "Now we will get into Multiworker strategy "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRCP9pbkslcG",
        "outputId": "5731f2bd-06c6-4b6a-ce09-2babbe39dd8d"
      },
      "source": [
        "strategy = tf.distribute.MultiWorkerMirroredStrategy()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
            "INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:CPU:0',), communication = CommunicationImplementation.AUTO\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSrJ4g0puob-",
        "outputId": "597844fa-c524-4d20-8f92-f47ba802578b"
      },
      "source": [
        "with strategy.scope():\n",
        "  multi_worker_model = mnist.build_and_compile_cnn_model()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wqro0j2WvUVF"
      },
      "source": [
        "To actually run with MultiWorkerMirroredStrategy you'll need to run worker processes and pass a TF_CONFIG to them.\n",
        "\n",
        "Like the mnist.py file written earlier, here is the main.py that each of the workers will run:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XbYXlpLvHCe",
        "outputId": "081d5849-f1d4-498f-9615-b4bdac7df5fe"
      },
      "source": [
        "%%writefile main.py \n",
        "\n",
        "import os \n",
        "import json \n",
        "\n",
        "import tensorflow as tf\n",
        "import mnist\n",
        "\n",
        "per_worker_batch_size = 64\n",
        "tf_config = json.loads(os.environ['TF_CONFIG'])\n",
        "num_workers = len(tf_config['cluster']['worker'])\n",
        "\n",
        "strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
        "\n",
        "global_batch_size = per_worker_batch_size*num_workers\n",
        "multi_worker_dataset = mnist.mnist_dataset(global_batch_size)\n",
        "\n",
        "with strategy.scope():\n",
        "  multi_worker_model = mnist.build_and_compile_cnn_model()\n",
        "\n",
        "multi_worker_model.fit(multi_worker_dataset, epoch = 3, steps_per_epoch= 70)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing main.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBi1eXdDxcOK"
      },
      "source": [
        "# json serialize the TF_CONFIG and add it to the environment variables\n",
        "os.environ['TF_CONFIG'] = json.dumps(tf_config)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gNUQb1u0OoU"
      },
      "source": [
        "Now, you can launch a worker process that will run the main.py and use the TF_CONFIG:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsfbK0sxx4xH",
        "outputId": "f49a1e75-b190-4263-aa33-10cd5157417d"
      },
      "source": [
        "# first kill any previous runs \n",
        "\n",
        "%killbgscripts"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All background processes were killed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jhNq32-0VoZ"
      },
      "source": [
        "#We will use the below command to run main.py file and to log hte outputs to a log file \n",
        "! python main.py &> job_0.log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jx5Z9NDx15O6"
      },
      "source": [
        "import time\n",
        "time.sleep(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7K-98I1x3sC7"
      },
      "source": [
        "# look over to the log file \n",
        "! cat job_0.log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EP5IzmDT5ZTZ"
      },
      "source": [
        "The first worker is ready and is waiting for all the other workers to be ready to proceed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQSojbAe5IRJ"
      },
      "source": [
        "# update the tf_config for the second worker's process to pick up\n",
        "tf_config['task']['index'] = 1 \n",
        "os.environ['TF_CONFIG'] = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSkIwKTX6no1"
      },
      "source": [
        "Now launch the second worker. This will start the training since all the workers are active (so there's no need to background this process):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2LN9g7Z59an"
      },
      "source": [
        "# launching second worker \n",
        "! python main.py "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3SDsIuY69G9"
      },
      "source": [
        "This will run slower than the last time because we running this on a single machine. We will kill the background so that they dont affect any next process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UT7nsEgt8F-F"
      },
      "source": [
        "os.environ.pop('TF_CONFIG', None)\n",
        "%killbgscripts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGm_Ipai9Phs"
      },
      "source": [
        "## Dataset Sharding \n",
        "In multi-worker training, dataset sharding is needed to ensure convergence and performance.\n",
        "\n",
        "The example in the previous section relies on the default autosharding provided by the tf.distribute.Strategy API. You can control the sharding by setting the tf.data.experimental.AutoShardPolicy of the tf.data.experimental.DistributeOptions. To learn more about auto-sharding see the Distributed input guide.\n",
        "\n",
        "Here is a quick example of how to turn OFF the auto sharding, so each replica processes every example (not recommended):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0uX9IZq9bEk"
      },
      "source": [
        "options = tf.data.Options()\n",
        "options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n",
        "\n",
        "\n",
        "global_batch_size = 64\n",
        "multi_worker_dataset = mnist.mnist_dataset(batch_size = 64)\n",
        "dataset_no_auto_shard = multi_worker_dataset.with_options(options)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUfhr8DKBCGm"
      },
      "source": [
        "## Model saving \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpshSF8ZBPNr"
      },
      "source": [
        "model_path = '/tmp/keras_model'\n",
        "\n",
        "def _is_chief(task_type, task_id):\n",
        "  # as this task will run with a single worker, ww'll add taskype=none\n",
        "  # this is how the tf architecutre is designed \n",
        "\n",
        "  return (task_type == 'worker' and task_id == 0) or task_type is None\n",
        "\n",
        "def _get_temp_dir(dirpath, task_id):\n",
        "  base_dirpath = 'workertemp_' + str(task_id)\n",
        "  temp_dir = os.path.join(dirpath, base_dirpath)\n",
        "  tf.io.gfile.makedirs(temp_dir)\n",
        "  return temp_dir \n",
        "\n",
        "def write_filepath(filepath, task_type, task_id):\n",
        "  dirpath = os.path.dirname(filepath)\n",
        "  base = os.path.basename(filepath)\n",
        "  if not _is_chief(task_type, task_id):\n",
        "    dirpath = _get_temp_dir(dirpath, task_id)\n",
        "  return os.path.join(dirpath, base)\n",
        "\n",
        "task_type, task_id = (strategy.cluster_resolver.task_type,\n",
        "                      strategy.cluster_resolver.task_id)\n",
        "\n",
        "write_model_path = write_filepath(model_path, task_type, task_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imEzDuHSCGjI"
      },
      "source": [
        "# model saving \n",
        "multi_worker_model.save(write_model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbDCI9d8EbZB"
      },
      "source": [
        "As described above, later on the model should only be loaded from the path chief saved to, so let's remove the temporary ones the non-chief workers saved:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x47GxqTcEbvM"
      },
      "source": [
        "if not _is_chief(task_type, task_id):\n",
        "  tf.io.gfile.rmtree(os.path.dirname(write_model_path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHXTowzrFNyF"
      },
      "source": [
        "Now, when it's time to load, let's use convenient tf.keras.models.load_model API, and continue with further work. Here, assume only using single worker to load and continue training, in which case you do not call tf.keras.models.load_model within another strategy.scope()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbbFh2xnFS9c"
      },
      "source": [
        "loaded_model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "loaded_model.fit(single_worker_dataset, epochs = 2, steps_per_epoch = 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JV7G4MSzF7BH"
      },
      "source": [
        "##Checkpoint saving and restoring\n",
        "\n",
        "On the other hand, checkpointing allows you to save model's weights and restore them without having to save the whole model. Here, you'll create one tf.train.Checkpoint that tracks the model, which is managed by a tf.train.CheckpointManager so that only the latest checkpoint is preserved."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9jkRopeF8qe"
      },
      "source": [
        "checkpoint_dir = '/tmp/ckpt'\n",
        "\n",
        "checkpoint = tf.train.Checkpoint(model = multi_worker_model)\n",
        "write_checkpoint_dir = write_filepath(checkpoint_dir, task_type, task_id)\n",
        "checkpoint_manager = tf.train.CheckpointManager(\n",
        "    checkpoint, directory = write_checkpoint_dir, max_to_keep = 1\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naDna5j-G2Jc"
      },
      "source": [
        "Once the CheckpointManager is set up, you're now ready to save, and remove the checkpoints non-chief workers saved."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYL13qxgG6z3"
      },
      "source": [
        "checkpoint_manager.save():\n",
        "if not _is_chief(task_type, task_id):\n",
        "  tf.io.gfile.rmtree(write_checkpoint_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3X07KVv-HOYV"
      },
      "source": [
        "Now, when you need to restore, you can find the latest checkpoint saved using the convenient tf.train.latest_checkpoint function. After restoring the checkpoint, you can continue with training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1Qn6Ze1HRBl"
      },
      "source": [
        "latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "checkpoint.restore(latest_checkpoint)\n",
        "multi_worker_model.fit(multi_worker_dataset, epochs = 2, steps_per_epoch = 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hj-8itLzHtgX"
      },
      "source": [
        "## Backup and restore callback \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9pO0AgsIUQT"
      },
      "source": [
        "callbacks = [tf.keras.callbacks.experimental.BackupAndRestore(backup_dir = '/tmp/backup')]\n",
        "\n",
        "with strategy.scope():\n",
        "  multi_worker_model = mnist.build_and_compile_cnn_model()\n",
        "\n",
        "multi_worker_model.fit(multi_worker_dataset,\n",
        "                       epochs = 3,\n",
        "                       steps_per_epoch = 70,\n",
        "                       callbacks = callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}